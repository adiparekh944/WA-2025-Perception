The processing pipeline begins by selecting bounding boxes and converting them into pixel coordinates `(u, v)`. Around these coordinates, a small region of the 3D point cloud is extracted to help reduce noise. The median of the valid points within this patch is calculated to provide a estimate of the `(X, Y, Z)` position. To improve accuracy, extreme values in each axis are removed using percentile thresholds, and filtering is applied to smooth the trajectory and reduce jitter. The coordinates are then converted from the camera frame to the world frame, and further transformed into a BEV for visualization. Finally, the trajectory is rescaled to align the starting position with the expected initial distance.

The results include an animated plot saved as an MP4 file that shows the trajectory evolving over time, as well as a static plot in PNG format that displays the full trajectory with markers indicating the start, end, and direction. The analysis part of the project calculates several metrics such as the number of valid trajectory samples, the total path length and displacement, the turn directiond, and the distance from the vehicle to the traffic light at both the start and end of the trajectory.

Several assumptions are made throughout the process: the bounding box CSV files and `.npz` point cloud files are aligned by frame, the traffic light is used as the origin of the world coordinate system, the initial longitudinal distance is roughly 14 meters, and the vehicle path is expected to be relatively smooth without abrupt jumps.

The final results include the files `trajectory.mp4` for the animated view and `trajectory_output.png` for the static view of the path.